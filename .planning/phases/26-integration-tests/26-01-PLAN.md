---
phase: 26-integration-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [tests/intg-json-output.bats]
autonomous: true

must_haves:
  truths:
    - "Every one of the 46 use-case scripts passes a BATS integration test that validates its -j output is parseable JSON"
    - "The JSON envelope from each script contains required keys: meta.tool, meta.script, meta.started, results (array), summary.total, summary.succeeded, summary.failed"
    - "All new integration tests pass in CI alongside the existing 295-test suite (bats tests/ exits 0)"
  artifacts:
    - path: "tests/intg-json-output.bats"
      provides: "Dynamic integration tests for JSON output of all 46 use-case scripts"
      min_lines: 80
      contains: "_test_json_output"
  key_links:
    - from: "tests/intg-json-output.bats"
      to: "scripts/*/[use-case].sh"
      via: "dynamic discovery with find + bats_test_function registration"
      pattern: "_discover_json_scripts"
    - from: "tests/intg-json-output.bats"
      to: "scripts/lib/json.sh"
      via: "validates JSON envelope produced by json_finalize"
      pattern: "jq -e"
---

<objective>
Create BATS integration tests that exercise every one of the 46 use-case scripts with the `-j` flag and validate the JSON output is both parseable and structurally correct.

Purpose: Automated regression detection -- ensures no script's JSON output breaks silently when changes are made to lib/json.sh, lib/args.sh, or individual scripts.

Output: `tests/intg-json-output.bats` containing ~49 tests (46 dynamically registered per-script tests + 2-3 static meta-tests).
</objective>

<execution_context>
@/Users/patrykattc/.claude/get-shit-done/workflows/execute-plan.md
@/Users/patrykattc/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Blueprint: existing integration test file with identical infrastructure patterns
@tests/intg-cli-contracts.bats

# JSON library (json_finalize writes to fd3 = original stdout)
@scripts/lib/json.sh

# Args library (parse_common_args does exec 3>&1; exec 1>&2 for -j mode)
@scripts/lib/args.sh

# Research with verified patterns and complete code example
@.planning/phases/26-integration-tests/26-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create intg-json-output.bats with dynamic JSON output tests</name>
  <files>tests/intg-json-output.bats</files>
  <action>
Create `tests/intg-json-output.bats` following the exact pattern from `tests/intg-cli-contracts.bats`. The research provides a near-complete implementation in the "Complete intg-json-output.bats File Structure" section. Key elements:

**File-level setup (parse time):**
- `PROJECT_ROOT="$(git rev-parse --show-toplevel)"`
- `_discover_json_scripts()` -- uses `find` to discover use-case scripts, excluding: `*/lib/*`, `common.sh`, `check-docs-completeness.sh`, `*/diagnostics/*`, `check-tools.sh`, `examples.sh`. This targets the 46 use-case scripts only.

**Test function `_test_json_output()`:**
- Takes script path as argument
- Runs: `run bash "$script" -j dummy_target 2>/dev/null`
- `assert_success` (catches missing mock tools early)
- `echo "$output" | jq -e '.'` -- valid JSON (TEST-04)
- `echo "$output" | jq -e 'has("meta") and has("results") and has("summary")'` -- top-level keys
- `echo "$output" | jq -e '.meta | has("tool") and has("script") and has("started")'` -- meta keys (TEST-03)
- `echo "$output" | jq -e '.meta.tool | type == "string" and length > 0'` -- tool is non-empty string
- `echo "$output" | jq -e '.meta.script | type == "string" and length > 0'` -- script is non-empty string
- `echo "$output" | jq -e '.results | type == "array"'` -- results is array
- `echo "$output" | jq -e '.summary | has("total") and has("succeeded") and has("failed")'` -- summary keys
- `echo "$output" | jq -e '.summary.total | type == "number"'` -- total is numeric

**Dynamic registration:**
- `while IFS= read -r script; do ... bats_test_function --description "JSON-01 ${local_path}: -j produces valid JSON with correct envelope" -- _test_json_output "$script"; done < <(_discover_json_scripts)`

**Static meta-tests:**
- `@test "JSON-META: discovery finds at least 46 use-case scripts"` -- `assert [ "$count" -ge 46 ]` (use -ge, not -eq, per research pitfall #7)
- `@test "JSON-META: jq is available for JSON validation"` -- `run command -v jq; assert_success`

**File-level lifecycle (setup_file/teardown_file/setup):**
- Copy the mock tool and wordlist infrastructure verbatim from `intg-cli-contracts.bats`. Same 18 mock tools: nmap, tshark, msfconsole, msfvenom, aircrack-ng, hashcat, skipfish, sqlmap, hping3, john, nikto, foremost, dig, curl, nc, traceroute, mtr, gobuster, ffuf. Same 3 wordlists: common.txt, subdomains-top1million-5000.txt, rockyou.txt.
- `setup()` loads `test_helper/common-setup` and prepends `MOCK_BIN` to PATH.

**Important: Do NOT pass `-x` flag in tests.** Only `-j` is needed. Show mode is sufficient for JSON validation and avoids executing real commands.
  </action>
  <verify>
Run the new test file in isolation to verify all tests pass:
```bash
bats tests/intg-json-output.bats
```
Expected: 48-49 tests pass (46 dynamic + 2-3 static). Zero failures.
  </verify>
  <done>All 46 use-case scripts have a passing integration test validating their -j output is parseable JSON with correct envelope structure (meta.tool, meta.script, meta.started, results array, summary.total/succeeded/failed).</done>
</task>

<task type="auto">
  <name>Task 2: Verify full test suite passes with new integration tests</name>
  <files>tests/intg-json-output.bats</files>
  <action>
Run the complete test suite (`bats tests/`) to verify:
1. The new intg-json-output.bats tests pass alongside all existing tests
2. No existing tests are broken by the new file
3. Total test count is at least 295 (existing) + 48 (new) = 343+

If any tests fail:
- If a specific script fails `assert_success`: check that the mock tool list covers the tool that script requires. Add it to the mock list if missing.
- If `jq -e` fails: run the failing script manually with `bash scripts/tool/script.sh -j dummy_target 2>/dev/null | jq .` to diagnose the JSON issue. Fix the script or test as needed.
- If the count meta-test fails: adjust the `-ge` threshold if new scripts were added since the count was established.

Do NOT modify existing test files. The new file is purely additive.
  </action>
  <verify>
```bash
bats tests/
```
Expected: All tests pass (343+ total). Exit code 0.
  </verify>
  <done>The full test suite passes with zero failures. The new integration tests coexist with the existing 295 tests.</done>
</task>

</tasks>

<verification>
1. `bats tests/intg-json-output.bats` -- all ~49 tests pass (46 dynamic JSON tests + 2-3 meta-tests)
2. `bats tests/` -- all 343+ tests pass (existing 295 + new ~49)
3. `bats tests/intg-json-output.bats --count` -- returns at least 48
4. Spot-check: `bash scripts/nmap/discover-live-hosts.sh -j dummy_target 2>/dev/null | jq .` produces valid JSON with meta/results/summary keys
</verification>

<success_criteria>
- tests/intg-json-output.bats exists with dynamic test registration for all 46 use-case scripts
- Every script's `-j` output is validated for: valid JSON, required envelope keys (meta.tool, meta.script, meta.started, results, summary.total/succeeded/failed)
- `bats tests/` passes with 0 failures (existing + new tests together)
</success_criteria>

<output>
After completion, create `.planning/phases/26-integration-tests/26-01-SUMMARY.md`
</output>
